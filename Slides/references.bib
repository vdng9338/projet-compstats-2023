
@misc{davidson_hyperspherical_2022,
	title = {Hyperspherical Variational Auto-Encoders},
	url = {http://arxiv.org/abs/1804.00891},
	abstract = {The Variational Auto-Encoder ({VAE}) is one of the most used unsupervised machine learning models. But although the default choice of a Gaussian distribution for both the prior and posterior represents a mathematically convenient distribution often leading to competitive results, we show that this parameterization fails to model data with a latent hyperspherical structure. To address this issue we propose using a von Mises-Fisher ({vMF}) distribution instead, leading to a hyperspherical latent space. Through a series of experiments we show how such a hyperspherical {VAE}, or \${\textbackslash}mathcal\{S\}\$-{VAE}, is more suitable for capturing data with a hyperspherical latent structure, while outperforming a normal, \${\textbackslash}mathcal\{N\}\$-{VAE}, in low dimensions on other data types. Code at http://github.com/nicola-decao/s-vae-tf and https://github.com/nicola-decao/s-vae-pytorch},
	number = {{arXiv}:1804.00891},
	publisher = {{arXiv}},
	author = {Davidson, Tim R. and Falorsi, Luca and De Cao, Nicola and Kipf, Thomas and Tomczak, Jakub M.},
	urldate = {2023-10-19},
	date = {2022-09-27},
	eprinttype = {arxiv},
	eprint = {1804.00891 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {arXiv.org Snapshot:C\:\\Users\\INES\\Zotero\\storage\\8HD375VW\\1804.html:text/html;Full Text PDF:C\:\\Users\\INES\\Zotero\\storage\\UDPJHDJS\\Davidson et al. - 2022 - Hyperspherical Variational Auto-Encoders.pdf:application/pdf},
}

@article{guan_structure--motion_2017,
	title = {Structure-From-Motion in Spherical Video Using the von Mises-Fisher Distribution},
	volume = {26},
	issn = {1057-7149, 1941-0042},
	url = {http://ieeexplore.ieee.org/document/7707455/},
	doi = {10.1109/TIP.2016.2621662},
	abstract = {In this paper, we present a complete pipeline for computing structure-from-motion from the sequences of spherical images. We revisit problems from multiview geometry in the context of spherical images. In particular, we propose methods suited to spherical camera geometry for the spherical-n-point problem (estimating camera pose for a spherical image) and calibrated spherical reconstruction (estimating the position of a 3-D point from multiple spherical images). We introduce a new probabilistic interpretation of spherical structure-from-motion which uses the von Mises-Fisher distribution to model noise in spherical feature point positions. This model provides an alternate objective function that we use in bundle adjustment. We evaluate our methods quantitatively and qualitatively on both synthetic and real world data and show that our methods developed for spherical images outperform straightforward adaptations of methods developed for perspective images. As an application of our method, we use the structure-from-motion output to stabilise the viewing direction in fully spherical video.},
	pages = {711--723},
	number = {2},
	journaltitle = {{IEEE} Transactions on Image Processing},
	shortjournal = {{IEEE} Trans. on Image Process.},
	author = {Guan, Hao and Smith, William A. P.},
	urldate = {2023-11-07},
	date = {2017-02},
	langid = {english},
	file = {Guan et Smith - 2017 - Structure-From-Motion in Spherical Video Using the.pdf:C\:\\Users\\INES\\Zotero\\storage\\68545RMS\\Guan et Smith - 2017 - Structure-From-Motion in Spherical Video Using the.pdf:application/pdf},
}

@misc{doersch_tutorial_2021,
	title = {Tutorial on Variational Autoencoders},
	url = {http://arxiv.org/abs/1606.05908},
	abstract = {In just three years, Variational Autoencoders ({VAEs}) have emerged as one of the most popular approaches to unsupervised learning of complicated distributions. {VAEs} are appealing because they are built on top of standard function approximators (neural networks), and can be trained with stochastic gradient descent. {VAEs} have already shown promise in generating many kinds of complicated data, including handwritten digits, faces, house numbers, {CIFAR} images, physical models of scenes, segmentation, and predicting the future from static images. This tutorial introduces the intuitions behind {VAEs}, explains the mathematics behind them, and describes some empirical behavior. No prior knowledge of variational Bayesian methods is assumed.},
	number = {{arXiv}:1606.05908},
	publisher = {{arXiv}},
	author = {Doersch, Carl},
	urldate = {2023-11-08},
	date = {2021-01-03},
	eprinttype = {arxiv},
	eprint = {1606.05908 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {arXiv.org Snapshot:C\:\\Users\\INES\\Zotero\\storage\\AIGKL6B6\\1606.html:text/html;Full Text PDF:C\:\\Users\\INES\\Zotero\\storage\\Q4RQ4Z8F\\Doersch - 2021 - Tutorial on Variational Autoencoders.pdf:application/pdf},
}

@misc{kipf_variational_2016,
	title = {Variational Graph Auto-Encoders},
	url = {http://arxiv.org/abs/1611.07308},
	doi = {10.48550/arXiv.1611.07308},
	abstract = {We introduce the variational graph auto-encoder ({VGAE}), a framework for unsupervised learning on graph-structured data based on the variational auto-encoder ({VAE}). This model makes use of latent variables and is capable of learning interpretable latent representations for undirected graphs. We demonstrate this model using a graph convolutional network ({GCN}) encoder and a simple inner product decoder. Our model achieves competitive results on a link prediction task in citation networks. In contrast to most existing models for unsupervised learning on graph-structured data and link prediction, our model can naturally incorporate node features, which significantly improves predictive performance on a number of benchmark datasets.},
	number = {{arXiv}:1611.07308},
	publisher = {{arXiv}},
	author = {Kipf, Thomas N. and Welling, Max},
	urldate = {2023-11-08},
	date = {2016-11-21},
	eprinttype = {arxiv},
	eprint = {1611.07308 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {arXiv Fulltext PDF:C\:\\Users\\INES\\Zotero\\storage\\9K6LHV5T\\Kipf et Welling - 2016 - Variational Graph Auto-Encoders.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\INES\\Zotero\\storage\\UIQYVNSZ\\1611.html:text/html},
}

@misc{naesseth2020reparameterization,
      title={Reparameterization Gradients through Acceptance-Rejection Sampling Algorithms}, 
      author={Christian A. Naesseth and Francisco J. R. Ruiz and Scott W. Linderman and David M. Blei},
      year={2020},
      eprint={1610.05683},
      archivePrefix={arXiv},
      primaryClass={stat.ML}
}

@misc{paisley2012variational,
      title={Variational Bayesian Inference with Stochastic Search}, 
      author={John Paisley and David Blei and Michael Jordan},
      year={2012},
      eprint={1206.6430},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@misc{mnih2014neural,
      title={Neural Variational Inference and Learning in Belief Networks}, 
      author={Andriy Mnih and Karol Gregor},
      year={2014},
      eprint={1402.0030},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@article{McCallum_2000,
	title={Automating the Construction of Internet Portals with Machine Learning},
	volume={3},
	ISSN={1386-4564},
	url={http://dx.doi.org/10.1023/a:1009953814988},
	DOI={10.1023/a:1009953814988},
	number={2},
	journal={Information Retrieval},
	publisher={Springer Science and Business Media LLC},
	author={McCallum, Andrew Kachites and Nigam, Kamal and Rennie, Jason and Seymore, Kristie},
	year={2000},
	pages={127â€“163}
}